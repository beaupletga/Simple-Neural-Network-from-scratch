{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2)\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(x,y,derivative=False,name='mse'):\n",
    "    if name=='mse':\n",
    "        return mse(x,y,derivative)\n",
    "    if name=='mae':\n",
    "        return mae(x,y,derivative)\n",
    "\n",
    "def mse(x,y,derivative=False):\n",
    "    if not derivative:\n",
    "        return 0.5*np.mean(x-y)**2\n",
    "    else:\n",
    "        return (x-y)\n",
    "    \n",
    "def mae(x,y,derivative=False):\n",
    "    if not derivative:\n",
    "        return np.abs(x-y)\n",
    "    else:\n",
    "        return (x-y)/(((x-y)**2)**0.5)\n",
    "\n",
    "def show_error(error_list):\n",
    "    plt.clf()\n",
    "    plt.plot(error_list)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_initializer(size,name='random_normal'):\n",
    "    if name=='random_normal':\n",
    "        return np.random.normal(size=size)\n",
    "    if name=='random_uniform':\n",
    "        return np.random.uniform(size=size)\n",
    "    if name=='random':\n",
    "        return 2*np.random.random(size=size)-1\n",
    "    if name=='zeros':\n",
    "        return np.zeros(size)\n",
    "    if name=='ones':\n",
    "        return np.ones(size)\n",
    "    if name=='lecun_uniform':\n",
    "        limit=np.sqrt(3 / size[0])\n",
    "        return np.random.uniform(low=-limit,high=limit,size=size)\n",
    "    if name=='TruncatedNormal':\n",
    "        return np.random.normal(loc=0,scale=0.5,size=size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x,is_derivative=False,name='elu'):\n",
    "    if name=='relu':\n",
    "        return relu(x,is_derivative)\n",
    "    if name=='sigmoid':\n",
    "        return sigmoid(x,is_derivative)\n",
    "    if name=='tanh':\n",
    "        return tanh(x,is_derivative)\n",
    "    if name=='elu':\n",
    "        return elu(x,is_derivative)\n",
    "    \n",
    "def sigmoid(a,is_derivative=False):\n",
    "    x=copy.copy(a)\n",
    "    if is_derivative:\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))    \n",
    "\n",
    "def relu(a,is_derivative=False):\n",
    "    x=copy.copy(a)\n",
    "    if not is_derivative:\n",
    "        return np.maximum(x,0)\n",
    "    else:\n",
    "        x[x>=0]=1\n",
    "        x[x<0]=0\n",
    "        return x\n",
    "def tanh(a,is_derivative=False):\n",
    "    x=copy.copy(a)\n",
    "    if not is_derivative:\n",
    "        return np.tanh(x)\n",
    "    else:\n",
    "        return 1-x**2\n",
    "    \n",
    "def elu(a,is_derivative=False):\n",
    "    alpha=1\n",
    "    x=copy.copy(a)\n",
    "    if not is_derivative:\n",
    "        x[x<0]=alpha*(np.exp(x[x<0])-1)\n",
    "        return x\n",
    "    else:\n",
    "        x[x>=0]=1\n",
    "        x[x<0]=x[x<0]+alpha\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regularizer(weights_list,coeff,name='l2'):\n",
    "    if name=='l2':\n",
    "        return l2_regularizer(weights_list,coeff)\n",
    "    elif name=='l1':\n",
    "        return l1_regularizer(weights_list,coeff)\n",
    "\n",
    "def l2_regularizer(weights_list,coeff):\n",
    "    sum=0\n",
    "    for i in weights_list:\n",
    "        sum+=np.sum(i**2)\n",
    "    return coeff*sum\n",
    "\n",
    "def l1_regularizer(weights_list,coeff):\n",
    "    sum=0\n",
    "    for i in weights_list:\n",
    "        sum+=np.sum(np.abs(i))\n",
    "    return coeff*sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop(a,drop_name='dropout',ratio=0.25):\n",
    "    if drop_name==None:\n",
    "        return a\n",
    "    if drop_name=='dropout':\n",
    "        return dropout(a,ratio)\n",
    "    elif dop_name==\"dc\":\n",
    "        return drop_connect(a,ratio)\n",
    "\n",
    "# we set randomly connections to zeros\n",
    "def drop_connect(a,ratio=0.05):\n",
    "    x=copy.copy(a)\n",
    "    rand=np.random.random(list(x.shape))\n",
    "    x[rand<ratio]=0\n",
    "    return x\n",
    "\n",
    "# we set to zeros one of the column of the weights matrix\n",
    "def dropout(a,ratio):\n",
    "    x=copy.copy(a)\n",
    "    rand=np.random.rand(x.shape[1])\n",
    "    x[:,rand<ratio]=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.78753675e-04]\n",
      " [  9.99821246e-01]\n",
      " [  9.99821246e-01]\n",
      " [ -1.78753675e-04]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLn8QvygMrAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f76c0bcd2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def neural_network(x,y,epochs,hidden=[2,3],bias=True,gamma=0.1,drop_name=None,loss_name='mse',\n",
    "                  weights_init_name='random_normal',lr_decay=0,regularizer_name='l2',reg_coeff=1e-5):\n",
    "    length=len(hidden)\n",
    "    error_list=[]\n",
    "    \n",
    "#     initialize the weigths for consistant matrix multiplications\n",
    "    weights_list=[weights_initializer((x.shape[1],hidden[0]),weights_init_name)]\n",
    "    for i in range(1,length):\n",
    "        weights_list.append(weights_initializer((hidden[i-1],hidden[i]),weights_init_name))\n",
    "    weights_list.append(weights_initializer((hidden[-1],y.shape[1]),weights_init_name))\n",
    "    \n",
    "#     add bias \n",
    "#     there are as many bias as hidden_layer+1\n",
    "    if bias:\n",
    "        bias=2*np.random.rand(length+1)-1\n",
    "    else:\n",
    "        bias=np.zeros(length+1)\n",
    "        \n",
    "    for iter in range(epochs):\n",
    "        gamma*=1/(1+lr_decay*epochs)\n",
    "#         list containing the output of each layer\n",
    "#         len(out_list)=len(hidden)+1\n",
    "        out_list=[]\n",
    "        for j in range(0,length+1):\n",
    "#             if first layer the first element is x\n",
    "            if j==0:\n",
    "                out_list.append(f(x.dot(drop(weights_list[0],drop_name))+bias[j]))\n",
    "#             else this is the result of the previous layer\n",
    "            else:\n",
    "#                we don t apply dropout to the last layer because we might annul one of the outputs neurons   \n",
    "                if j!=length:\n",
    "                    out_list.append(f(out_list[-1].dot(drop(weights_list[j],drop_name))+bias[j]))\n",
    "                else:\n",
    "                    out_list.append(f(out_list[-1].dot(weights_list[j])+bias[j]))\n",
    "#         print out_list\n",
    "#         compute the error of the algorithm (for the error curve)        \n",
    "        error = loss(out_list[-1],y,loss_name)+regularizer(weights_list,reg_coeff,regularizer_name)\n",
    "#         compute the first two partial derivative a the thumb rule\n",
    "        delta_list=[(loss(out_list[-1],y,True,loss_name)+regularizer(weights_list,reg_coeff,regularizer_name))*f(out_list[-1],True)]\n",
    "#         print out_list\n",
    "        for j in range(length,-1,-1):\n",
    "#             print out_list\n",
    "#             we use the previous result the previous delta\n",
    "#             we then multiply it by the weights of the next layer\n",
    "            delta_list.append(delta_list[-1].dot(weights_list[j].T)*f(out_list[j-1],True))\n",
    "        \n",
    "#         update the weights between each layer \n",
    "#         there are (lengths+1) weights matrix\n",
    "        for j in range(0,length+1):\n",
    "#             if this is the first weights matrix, then the input isn t the result of the previous layer but x (input data)\n",
    "            if j==0:\n",
    "                weights_list[j] -= gamma* x.T.dot(delta_list[length])\n",
    "#             if this is not the first weights matrix, then the input is the result of the previous layer\n",
    "            else:\n",
    "                weights_list[j] -= gamma* out_list[j-1].T.dot(delta_list[length-j])\n",
    "        \n",
    "        for j in range(length,-1,-1):\n",
    "            bias[j] -= gamma * np.mean(delta_list[j+1])\n",
    "#         we append each error to a list to see the evolution of the error\n",
    "        error_list.append(error)\n",
    "    print out_list[-1]\n",
    "#     show the error with pyplot\n",
    "    show_error(np.mean(error_list,axis=1))\n",
    "\n",
    "x=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y=np.array([[0,1,1,0]]).T\n",
    "\n",
    "neural_network(x,y,hidden=[4,2],epochs=1000,gamma=0.1,bias=True,drop_name=None,loss_name='mse',\n",
    "               weights_init_name='lecun_uniform',lr_decay=0,regularizer_name='l2',reg_coeff=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
