{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2)\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=(1,2)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(x,y,derivative=False,name='mse'):\n",
    "    if name=='mse':\n",
    "        return mse(x,y,derivative)\n",
    "    if name=='mae':\n",
    "        return mae(x,y,derivative)\n",
    "\n",
    "def mse(x,y,derivative=False):\n",
    "    if not derivative:\n",
    "        return 0.5*np.mean(x-y)**2\n",
    "    else:\n",
    "        return (x-y)\n",
    "    \n",
    "def mae(x,y,derivative=False):\n",
    "    if not derivative:\n",
    "        return np.abs(x-y)\n",
    "    else:\n",
    "        return (x-y)/(((x-y)**2)**0.5)\n",
    "\n",
    "def show_error(error_list):\n",
    "    plt.clf()\n",
    "    plt.plot(error_list)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_initializer(size,name='random_normal'):\n",
    "    if name=='random_normal':\n",
    "        return np.random.normal(size=size)\n",
    "    if name=='random_uniform':\n",
    "        return np.random.uniform(size=size)\n",
    "    if name=='random':\n",
    "        return 2*np.random.random(size=size)-1\n",
    "    if name=='zeros':\n",
    "        return np.zeros(size)\n",
    "    if name=='ones':\n",
    "        return np.ones(size)\n",
    "    if name=='lecun_uniform':\n",
    "        limit=np.sqrt(3 / size[0])\n",
    "        return np.random.uniform(low=-limit,high=limit,size=size)\n",
    "    if name=='TruncatedNormal':\n",
    "        return np.random.normal(loc=0,scale=0.5,size=size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x,is_derivative=False,name='elu'):\n",
    "    if name=='relu':\n",
    "        return relu(x,is_derivative)\n",
    "    if name=='sigmoid':\n",
    "        return sigmoid(x,is_derivative)\n",
    "    if name=='tanh':\n",
    "        return tanh(x,is_derivative)\n",
    "    if name=='elu':\n",
    "        return elu(x,is_derivative)\n",
    "    \n",
    "def sigmoid(a,is_derivative=False):\n",
    "    x=copy.copy(a)\n",
    "    if is_derivative:\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))    \n",
    "\n",
    "def relu(a,is_derivative=False):\n",
    "    x=copy.copy(a)\n",
    "    if not is_derivative:\n",
    "        return np.maximum(x,0)\n",
    "    else:\n",
    "        x[x>=0]=1\n",
    "        x[x<0]=0\n",
    "        return x\n",
    "def tanh(a,is_derivative=False):\n",
    "    x=copy.copy(a)\n",
    "    if not is_derivative:\n",
    "        return np.tanh(x)\n",
    "    else:\n",
    "        return 1-x**2\n",
    "    \n",
    "def elu(a,is_derivative=False):\n",
    "    alpha=1\n",
    "    x=copy.copy(a)\n",
    "    if not is_derivative:\n",
    "        x[x<0]=alpha*(np.exp(x[x<0])-1)\n",
    "        return x\n",
    "    else:\n",
    "        x[x>=0]=1\n",
    "        x[x<0]=x[x<0]+alpha\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop(a,drop_name='dropout',ratio=0.25):\n",
    "    if drop_name==None:\n",
    "        return a\n",
    "    if drop_name=='dropout':\n",
    "        return dropout(a,ratio)\n",
    "    elif dop_name==\"dc\":\n",
    "        return drop_connect(a,ratio)\n",
    "\n",
    "# we set randomly connections to zeros\n",
    "def drop_connect(a,ratio=0.05):\n",
    "    x=copy.copy(a)\n",
    "    rand=np.random.random(list(x.shape))\n",
    "    x[rand<ratio]=0\n",
    "    return x\n",
    "\n",
    "# we set to zeros one of the column of the weights matrix\n",
    "def dropout(a,ratio):\n",
    "    x=copy.copy(a)\n",
    "    rand=np.random.rand(x.shape[1])\n",
    "    x[:,rand<ratio]=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.88178420e-16]\n",
      " [  1.00000000e+00]\n",
      " [  1.00000000e+00]\n",
      " [  4.16333634e-16]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nktSFDH9J6kKGvyR1IcNfkrrQ/weV3kbpkLLFSwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f76c0c5f610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def neural_network(x,y,epochs,hidden=[2,3],bias=True,gamma=0.1,drop_name=None,loss_name='mse',\n",
    "                  weights_init_name='random_normal'):\n",
    "    length=len(hidden)\n",
    "    error_list=[]\n",
    "    \n",
    "#     initialize the weigths for consistant matrix multiplications\n",
    "    weights_list=[weights_initializer((x.shape[1],hidden[0]),weights_init_name)]\n",
    "    for i in range(1,length):\n",
    "        weights_list.append(weights_initializer((hidden[i-1],hidden[i]),weights_init_name))\n",
    "    weights_list.append(weights_initializer((hidden[-1],y.shape[1]),weights_init_name))\n",
    "    \n",
    "#     add bias \n",
    "#     there are as many bias as hidden_layer+1\n",
    "    if bias:\n",
    "        bias=2*np.random.rand(length+1)-1\n",
    "    else:\n",
    "        bias=np.zeros(length+1)\n",
    "        \n",
    "#     learning rate\n",
    "    for iter in range(epochs):\n",
    "#         list containing the output of each layer\n",
    "#         len(out_list)=len(hidden)+1\n",
    "        out_list=[]\n",
    "        for j in range(0,length+1):\n",
    "#             if first layer the first element is x\n",
    "            if j==0:\n",
    "                out_list.append(f(x.dot(drop(weights_list[0],drop_name))+bias[j]))\n",
    "#             else this is the result of the previous layer\n",
    "            else:\n",
    "#                we don t apply dropout to the last layer because we might annul one of the outputs neurons   \n",
    "                if j!=length:\n",
    "                    out_list.append(f(out_list[-1].dot(drop(weights_list[j],drop_name))+bias[j]))\n",
    "                else:\n",
    "                    out_list.append(f(out_list[-1].dot(weights_list[j])+bias[j]))\n",
    "#         print out_list\n",
    "#         compute the error of the algorithm (for the error curve)        \n",
    "        error = np.mean(loss(out_list[-1],y,loss_name))\n",
    "        \n",
    "#         compute the first two partial derivative a the thumb rule\n",
    "        delta_list=[loss(out_list[-1],y,True,loss_name)*f(out_list[-1],True)]\n",
    "#         print out_list\n",
    "        for j in range(length,-1,-1):\n",
    "#             print out_list\n",
    "#             we use the previous result the previous delta\n",
    "#             we then multiply it by the weights of the next layer\n",
    "            delta_list.append(delta_list[-1].dot(weights_list[j].T)*f(out_list[j-1],True))\n",
    "        \n",
    "#         update the weights between each layer \n",
    "#         there are (lengths+1) weights matrix\n",
    "        for j in range(0,length+1):\n",
    "#             if this is the first weights matrix, then the input isn t the result of the previous layer but x (input data)\n",
    "            if j==0:\n",
    "                weights_list[j] -= gamma* x.T.dot(delta_list[length])\n",
    "#             if this is not the first weights matrix, then the input is the result of the previous layer\n",
    "            else:\n",
    "                weights_list[j] -= gamma* out_list[j-1].T.dot(delta_list[length-j])\n",
    "        \n",
    "        for j in range(length,-1,-1):\n",
    "            bias[j] -= gamma * np.mean(delta_list[j+1])\n",
    "#         we append each error to a list to see the evolution of the error\n",
    "        error_list.append(error)\n",
    "    print out_list[-1]\n",
    "#     show the error with pyplot\n",
    "    show_error(error_list)\n",
    "\n",
    "x=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y=np.array([[0,1,1,0]]).T\n",
    "\n",
    "neural_network(x,y,hidden=[4,2],epochs=1000,gamma=0.1,bias=True,drop_name=None,loss_name='mse',weights_init_name='lecun_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
