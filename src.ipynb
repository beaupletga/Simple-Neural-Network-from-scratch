{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2)\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.27013357  0.33015928  0.24415638  0.42693669  0.84107904]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.eye(5)\n",
    "dropout(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse(x,y):\n",
    "    return 0.5*np.mean(x-y)**2\n",
    "\n",
    "def show_error(error_list):\n",
    "    plt.clf()\n",
    "    plt.plot(error_list)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x,is_derivative=False,name='elu'):\n",
    "    if name=='relu':\n",
    "        return relu(x,is_derivative)\n",
    "    if name=='sigmoid':\n",
    "        return sigmoid(x,is_derivative)\n",
    "    if name=='tanh':\n",
    "        return tanh(x,is_derivative)\n",
    "    if name=='elu':\n",
    "        return elu(x,is_derivative)\n",
    "    \n",
    "def sigmoid(a,is_derivative=False):\n",
    "    x=copy.copy(a)\n",
    "    if is_derivative:\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))    \n",
    "\n",
    "def relu(a,is_derivative=False):\n",
    "    x=copy.copy(a)\n",
    "    if not is_derivative:\n",
    "        return np.maximum(x,0)\n",
    "    else:\n",
    "        x[x>=0]=1\n",
    "        x[x<0]=0\n",
    "        return x\n",
    "def tanh(a,is_derivative=False):\n",
    "    x=copy.copy(a)\n",
    "    if not is_derivative:\n",
    "        return np.tanh(x)\n",
    "    else:\n",
    "        return 1-x**2\n",
    "    \n",
    "def elu(a,is_derivative=False):\n",
    "    alpha=1\n",
    "    x=copy.copy(a)\n",
    "    if not is_derivative:\n",
    "        x[x<0]=alpha*(np.exp(x[x<0])-1)\n",
    "        return x\n",
    "    else:\n",
    "        x[x>=0]=1\n",
    "        x[x<0]=x[x<0]+alpha\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop(a,drop_name='dropout',ratio=0.25):\n",
    "    if drop_name=='dropout':\n",
    "        return dropout(a,ratio)\n",
    "    elif dop_name==\"dc\":\n",
    "        return drop_connect(a,ratio)\n",
    "\n",
    "\n",
    "def drop_connect(a,ratio=0.05):\n",
    "    x=copy.copy(a)\n",
    "    rand=np.random.random(list(x.shape))\n",
    "    x[rand<ratio]=0\n",
    "    return x\n",
    "\n",
    "# we set to \n",
    "def dropout(a,ratio):\n",
    "    x=copy.copy(a)\n",
    "    rand=np.random.rand(x.shape[1])\n",
    "    x[:,rand<ratio]=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADhtJREFUeJzt3X+MZWV9x/H3xx0BgVZ2YUJW1nSXSGxIkxY6IRAa04Ai\nRSM0IQZj6lZpSPpTpYlC/cP0nwaqETE1ygY0m8YqiEQIsSUUsU3/WZ0VKrDLj3UR2M3CjlbU2CaV\n+O0f91m8bHd27s7MnTvz8H4lkznnOc+Z8z3z3P3Muc89d2+qCknS2veaSRcgSVoeBrokdcJAl6RO\nGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpE1MrebDTTjutNm/evJKHlKQ1b+fOnT+squmF+q1o\noG/evJnZ2dmVPKQkrXlJnhmln1MuktQJA12SOmGgS1InDHRJ6oSBLkmdMNAlqRMGuiR1Yk0F+pMv\n/IxvP/1fky5DklalFX1j0VJdctO/A/CDG94x4UokafVZU1fokqT5GeiS1ImRAj3Jh5M8luTRJF9O\nckKSLUl2JNmT5PYkx427WEnS/BYM9CRnAH8FzFTVbwHrgKuAG4GbqupNwI+Bq8dZqCTp6EadcpkC\nXpdkCjgROABcBNzZtm8Hrlj+8iRJo1ow0KtqP/BJ4FkGQf4TYCfwYlW91LrtA84YV5GSpIWNMuWy\nHrgc2AK8ATgJuHTUAyS5Jslsktm5ublFFypJOrpRplzeCjxdVXNV9QvgLuBC4JQ2BQOwCdh/pJ2r\naltVzVTVzPT0gh+4IUlapFEC/Vng/CQnJglwMbALeBC4svXZCtw9nhIlSaMYZQ59B4MXP78LPNL2\n2QZ8FLg2yR7gVOC2MdYpSVrASG/9r6qPAx8/rHkvcN6yVyRJWhTfKSpJnTDQJakTBrokdcJAl6RO\nGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHXCQJekThjoktQJA12SOmGgS1InDHRJ6oSB\nLkmdMNAlqRMGuiR1wkCXpE4Y6JLUCQNdkjphoEtSJwx0SeqEgS5JnTDQJakTBrokdcJAl6ROGOiS\n1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHXCQJekThjoktQJA12SOmGgS1InRgr0JKckuTPJ\n40l2J7kgyYYk9yd5qn1fP+5iJUnzG/UK/WbgX6rqN4HfBnYD1wEPVNVZwANtXZI0IQsGepLXA28B\nbgOoqv+tqheBy4Htrdt24IpxFSlJWtgoV+hbgDngi0keSnJrkpOA06vqQOvzPHD6uIqUJC1slECf\nAs4FPldV5wA/57DplaoqoI60c5JrkswmmZ2bm1tqvZKkeYwS6PuAfVW1o63fySDgX0iyEaB9P3ik\nnatqW1XNVNXM9PT0ctQsSTqCBQO9qp4Hnkvy5tZ0MbALuAfY2tq2AnePpUJJ0kimRuz3l8CXkhwH\n7AXez+CPwR1JrgaeAd49nhIlSaMYKdCr6mFg5gibLl7eciRJi+U7RSWpEwa6JHXCQJekThjoktQJ\nA12SOmGgS1InDHRJ6oSBLkmdMNAlqRMGuiR1wkCXpE4Y6JLUCQNdkjphoEtSJwx0SeqEgS5JnTDQ\nJakTBrokdcJAl6ROGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHXCQJekThjoktQJA12S\nOmGgS1InDHRJ6oSBLkmdMNAlqRMGuiR1wkCXpE4Y6JLUCQNdkjphoEtSJwx0SerEyIGeZF2Sh5Lc\n29a3JNmRZE+S25McN74yJUkLOZYr9A8Cu4fWbwRuqqo3AT8Grl7OwiRJx2akQE+yCXgHcGtbD3AR\ncGfrsh24YhwFSpJGM+oV+qeBjwC/bOunAi9W1UttfR9wxpF2THJNktkks3Nzc0sqVpI0vwUDPck7\ngYNVtXMxB6iqbVU1U1Uz09PTi/kRkqQRTI3Q50LgXUkuA04Afh24GTglyVS7St8E7B9fmZKkhSx4\nhV5V11fVpqraDFwFfLOq3gs8CFzZum0F7h5blZKkBS3lPvSPAtcm2cNgTv225SlJkrQYo0y5vKyq\nvgV8qy3vBc5b/pIkSYvhO0UlqRMGuiR1wkCXpE4Y6JLUCQNdkjphoEtSJwx0SeqEgS5JnTDQJakT\nBrokdcJAl6ROGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHXCQJekThjoktQJA12SOmGg\nS1InDHRJ6oSBLkmdMNAlqRMGuiR1wkCXpE4Y6JLUCQNdkjphoEtSJwx0SeqEgS5JnTDQJakTBrok\ndcJAl6ROGOiS1AkDXZI6YaBLUicMdEnqxIKBnuSNSR5MsivJY0k+2No3JLk/yVPt+/rxlytJms8o\nV+gvAX9dVWcD5wN/nuRs4Drggao6C3igrUuSJmTBQK+qA1X13bb8M2A3cAZwObC9ddsOXDGuIiVJ\nCzumOfQkm4FzgB3A6VV1oG16Hjh9nn2uSTKbZHZubm4JpUqSjmbkQE9yMvA14ENV9dPhbVVVQB1p\nv6raVlUzVTUzPT29pGIlSfMbKdCTvJZBmH+pqu5qzS8k2di2bwQOjqdESdIoRrnLJcBtwO6q+tTQ\npnuArW15K3D38pcnSRrV1Ah9LgT+CHgkycOt7W+AG4A7klwNPAO8ezwlSpJGsWCgV9V/AJln88XL\nW44kabF8p6gkdcJAl6ROGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHXCQJekThjoktQJ\nA12SOmGgS1InDHRJ6oSBLkmdMNAlqRMGuiR1wkCXpE4Y6JLUCQNdkjphoEtSJwx0SeqEgS5JnTDQ\nJakTBrokdcJAl6ROGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHXCQJekTqyZQH/2R/89\n6RIkaVVbE4H+5As/4y2feHDSZUjSqrYmAn3/j/9n0iVI0qq3JgJ93Wsy6RIkadWbWsrOSS4FbgbW\nAbdW1Q3LUtVhpg4L9E/c9/g4DiNJY/OBC7dw6snHj/UYiw70JOuAzwJvA/YB30lyT1XtWq7iDvn6\nw/tfsX7Lv+1d7kNI0lj94TmbVm+gA+cBe6pqL0CSrwCXA8se6HfM7nvF+p6/u2y5DyFJa95S5tDP\nAJ4bWt/X2iRJEzD2F0WTXJNkNsns3NzcuA8nSa9aS5ly2Q+8cWh9U2t7haraBmwDmJmZqcUc6Ac3\nvGMxu0nSq8pSrtC/A5yVZEuS44CrgHuWpyxJ0rFa9BV6Vb2U5C+A+xjctviFqnps2SqTJB2TJd2H\nXlXfAL6xTLVIkpZgTbxTVJK0MANdkjphoEtSJwx0SeqEgS5JnUjVot7rs7iDJXPAM4vc/TTgh8tY\nzlrgOb86eM79W+r5/kZVTS/UaUUDfSmSzFbVzKTrWEme86uD59y/lTpfp1wkqRMGuiR1Yi0F+rZJ\nFzABnvOrg+fcvxU53zUzhy5JOrq1dIUuSTqKNRHoSS5N8kSSPUmum3Q9xyLJG5M8mGRXkseSfLC1\nb0hyf5Kn2vf1rT1JPtPO9XtJzh36WVtb/6eSbB1q/90kj7R9PpMk/7+SlZdkXZKHktzb1rck2dHq\nvL39t8skOb6t72nbNw/9jOtb+xNJ3j7UvuoeE0lOSXJnkseT7E5yQe/jnOTD7XH9aJIvJzmht3FO\n8oUkB5M8OtQ29nGd7xhHVVWr+ovBf837feBM4DjgP4GzJ13XMdS/ETi3Lf8a8CRwNvD3wHWt/Trg\nxrZ8GfDPQIDzgR2tfQOwt31f35bXt23fbn3T9v2DSZ93q+ta4J+Ae9v6HcBVbfnzwJ+25T8DPt+W\nrwJub8tnt/E+HtjSHgfrVutjAtgO/ElbPg44pedxZvCRk08Drxsa3z/ubZyBtwDnAo8OtY19XOc7\nxlFrnfQ/ghF+mRcA9w2tXw9cP+m6lnA+dwNvA54ANra2jcATbfkW4D1D/Z9o298D3DLUfktr2wg8\nPtT+in4TPM9NwAPARcC97cH6Q2Dq8HFl8H/qX9CWp1q/HD7Wh/qtxscE8PoWbjmsvdtx5lefK7yh\njdu9wNt7HGdgM68M9LGP63zHONrXWphy6ebDqNtTzHOAHcDpVXWgbXoeOL0tz3e+R2vfd4T2Sfs0\n8BHgl239VODFqnqprQ/X+fK5te0/af2P9XcxSVuAOeCLbZrp1iQn0fE4V9V+4JPAs8ABBuO2k77H\n+ZCVGNf5jjGvtRDoXUhyMvA14ENV9dPhbTX4E9zN7UZJ3gkcrKqdk65lBU0xeFr+uao6B/g5g6fJ\nL+twnNcDlzP4Y/YG4CTg0okWNQErMa6jHmMtBPpIH0a9miV5LYMw/1JV3dWaX0iysW3fCBxs7fOd\n79HaNx2hfZIuBN6V5AfAVxhMu9wMnJLk0KdkDdf58rm17a8HfsSx/y4maR+wr6p2tPU7GQR8z+P8\nVuDpqpqrql8AdzEY+57H+ZCVGNf5jjGvtRDoa/rDqNsr1rcBu6vqU0Ob7gEOvdK9lcHc+qH297VX\ny88HftKedt0HXJJkfbsyuoTB/OIB4KdJzm/Het/Qz5qIqrq+qjZV1WYG4/XNqnov8CBwZet2+Dkf\n+l1c2fpXa7+q3R2xBTiLwQtIq+4xUVXPA88leXNruhjYRcfjzGCq5fwkJ7aaDp1zt+M8ZCXGdb5j\nzG+SL6ocwwsSlzG4O+T7wMcmXc8x1v57DJ4qfQ94uH1dxmDu8AHgKeBfgQ2tf4DPtnN9BJgZ+lkf\nAPa0r/cPtc8Aj7Z9/oHDXpib8Pn/Pr+6y+VMBv9Q9wBfBY5v7Se09T1t+5lD+3+sndcTDN3VsRof\nE8DvALNtrL/O4G6GrscZ+Fvg8VbXPzK4U6WrcQa+zOA1gl8weCZ29UqM63zHONqX7xSVpE6shSkX\nSdIIDHRJ6oSBLkmdMNAlqRMGuiR1wkCXpE4Y6JLUCQNdkjrxf/QZuTBViXJMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdfb82309d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def neural_network(x,y,epochs,hidden=[2,3],bias=True,gamma=0.1,drop_name='dropout'):\n",
    "    length=len(hidden)\n",
    "    error_list=[]\n",
    "    \n",
    "#     initialize the weigths for consistant matrix multiplications\n",
    "    weights_list=[2*np.random.rand(x.shape[1],hidden[0])-1]\n",
    "    for i in range(1,length):\n",
    "        weights_list.append(2*np.random.rand(hidden[i-1],hidden[i])-1)\n",
    "    weights_list.append(2*np.random.rand(hidden[-1],y.shape[1])-1)\n",
    "    \n",
    "#     add bias \n",
    "#     there are as many bias as hidden_layer+1\n",
    "    if bias:\n",
    "        bias=2*np.random.rand(length+1)-1\n",
    "    else:\n",
    "        bias=np.zeros(length+1)\n",
    "        \n",
    "#     learning rate\n",
    "    for iter in range(epochs):\n",
    "#         list containing the output of each layer\n",
    "#         len(out_list)=len(hidden)+1\n",
    "        out_list=[]\n",
    "        for j in range(0,length+1):\n",
    "#             if first layer the first element is x\n",
    "            if j==0:\n",
    "                out_list.append(f(x.dot(drop(weights_list[0],drop_name))+bias[j]))\n",
    "#             else this is the result of the previous layer\n",
    "            else:\n",
    "#                we don t apply dropout to the last layer because we might annul one of the outputs neurons   \n",
    "                if j!=length:\n",
    "                    out_list.append(f(out_list[-1].dot(drop(weights_list[j],drop_name))+bias[j]))\n",
    "                else:\n",
    "                    out_list.append(f(out_list[-1].dot(weights_list[j])+bias[j]))\n",
    "#         print out_list\n",
    "#         compute the error of the algorithm (for the error curve)        \n",
    "        error = mse(out_list[-1],y)\n",
    "        \n",
    "#         compute the first two partial derivative a the thumb rule\n",
    "        delta_list=[(out_list[-1]-y)*f(out_list[-1],True)]\n",
    "#         print out_list\n",
    "        for j in range(length,-1,-1):\n",
    "#             print out_list\n",
    "#             we use the previous result the previous delta\n",
    "#             we then multiply it by the weights of the next layer\n",
    "            delta_list.append(delta_list[-1].dot(weights_list[j].T)*f(out_list[j-1],True))\n",
    "        \n",
    "#         update the weights between each layer \n",
    "#         there are (lengths+1) weights matrix\n",
    "        for j in range(0,length+1):\n",
    "#             if this is the first weights matrix, then the input isn t the result of the previous layer but x (input data)\n",
    "            if j==0:\n",
    "                weights_list[j] -= gamma* x.T.dot(delta_list[length])\n",
    "#             if this is not the first weights matrix, then the input is the result of the previous layer\n",
    "            else:\n",
    "                weights_list[j] -= gamma* out_list[j-1].T.dot(delta_list[length-j])\n",
    "        \n",
    "        for j in range(length,-1,-1):\n",
    "            bias[j] -= gamma * np.mean(delta_list[j+1])\n",
    "#         we append each error to a list to see the evolution of the error\n",
    "        error_list.append(error)\n",
    "    print out_list[-1]\n",
    "#     show the error with pyplot\n",
    "    show_error(error_list)\n",
    "\n",
    "x=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y=np.array([[0,1,1,0]]).T\n",
    "\n",
    "neural_network(x,y,hidden=[4,2],epochs=100000,gamma=0.1,bias=True,drop_name='dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
